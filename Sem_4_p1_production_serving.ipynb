{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolaitpopov/C-/blob/main/Sem_4_p1_production_serving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8d17780c-d58d-4dbd-b680-b40298d6e99b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8d17780c-d58d-4dbd-b680-b40298d6e99b",
        "outputId": "61b81213-afef-4673-80e7-07276bfbd5d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a85d0b7-aa5b-45ce-885d-66f27c865fcf",
      "metadata": {
        "id": "2a85d0b7-aa5b-45ce-885d-66f27c865fcf"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21cc46ba-29ce-4283-99db-782dfa518820",
      "metadata": {
        "id": "21cc46ba-29ce-4283-99db-782dfa518820"
      },
      "source": [
        "В этой части занятия мы пройдем путь от простой идеи \"сделаю свой ChatGPT\" до понимания реальных вызовов и решений при развертывании LLM в продакшене\n",
        "\n",
        "**Что мы изучим:**\n",
        "- 🔥 Почему простого FastAPI + Transformers недостаточно\n",
        "- ⚡ Ключевые метрики производительности LLM\n",
        "- 🏗️ Архитектурные паттерны инференса\n",
        "- 🚄 Сравнение vLLM и TensorRT-LLM\n",
        "- 💰 Экономика self-hosting vs API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b4d8b5c-34c0-4ec0-ba19-97b2a1f8adac",
      "metadata": {
        "id": "6b4d8b5c-34c0-4ec0-ba19-97b2a1f8adac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import fastapi\n",
        "import uvicorn\n",
        "import transformers\n",
        "import tensorflow\n",
        "import numpy\n",
        "import pandas\n",
        "import accelerate\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d39d251d-4691-437a-8966-40ca02355a85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39d251d-4691-437a-8966-40ca02355a85",
        "outputId": "7782237b-0335-4ed9-d460-c7dbc75c53da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# версия питона\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ec2e02f3-3c78-4323-86f7-28df58fd2770",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec2e02f3-3c78-4323-86f7-28df58fd2770",
        "outputId": "c96e0b7a-795e-44cc-9e30-eda3c6dd786e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.6.0+cu124\n",
            "transformers 4.54.0\n",
            "tensorflow 2.18.0\n",
            "numpy 2.0.2\n",
            "pandas 2.2.2\n",
            "accelerate 1.9.0\n",
            "fastapi 0.116.1\n",
            "uvicorn 0.35.0\n"
          ]
        }
      ],
      "source": [
        "# список важных библиотек\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('tensorflow', tensorflow.__version__)\n",
        "print('numpy', numpy.__version__)\n",
        "print('pandas', pandas.__version__)\n",
        "print('accelerate', accelerate.__version__)\n",
        "print('fastapi', fastapi.__version__)\n",
        "print('uvicorn', uvicorn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ac69c8d1-7162-409c-9cae-f200b0105e53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac69c8d1-7162-409c-9cae-f200b0105e53",
        "outputId": "799f68ed-7a1b-4b11-cf18-61576a9ab979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPU(s).\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# информация о GPU для этого урока\n",
        "if torch.cuda.is_available():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Found {num_gpus} GPU(s).\")\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        print(f\"GPU {i}: {gpu_name}\")\n",
        "else:\n",
        "  print(\"CUDA is not available!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9peXiy3BhPv",
        "outputId": "36a788be-2d98-487e-b96e-de37a05752c7"
      },
      "id": "P9peXiy3BhPv",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n",
            "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.34.1\n",
            "    Uninstalling huggingface-hub-0.34.1:\n",
            "      Successfully uninstalled huggingface-hub-0.34.1\n",
            "Successfully installed huggingface_hub-0.34.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc38ea2-4acc-4520-8a49-26fc9b1d6634",
      "metadata": {
        "id": "3fc38ea2-4acc-4520-8a49-26fc9b1d6634"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"docs/meme.jpg\" alt=\"alt text\" style=\"width:400px;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc36858d-a409-470b-a92f-c263a997dd3b",
      "metadata": {
        "id": "cc36858d-a409-470b-a92f-c263a997dd3b"
      },
      "source": [
        "# Часть 1\n",
        "Здесь мы разберем следующие темы\n",
        "- 🔥 Почему простого FastAPI + Transformers недостаточно\n",
        "- ⚡ Ключевые метрики производительности LLM\n",
        "## 📦 Загрузка модели\n",
        "\n",
        "**Размеры современных моделей:**\n",
        "- Llama-3.2-1B: ~5 GB\n",
        "- Llama-3.1-8B: ~16 GB\n",
        "- Llama-3.1-70B: ~140 GB\n",
        "- GPT-4 уровень: 500GB+ (по оценкам)\n",
        "\n",
        "**Проблемы при загрузке:**\n",
        "1. **Время загрузки**: Даже 1B модель загружается минуты\n",
        "2. **Требования к RAM**: Модель должна поместиться в GPU памяти\n",
        "3. **Версионирование**: Разные версии transformers могут быть несовместимы\n",
        "4. **Токенизатор**: Отдельно нужно загружать и правильно настраивать\n",
        "\n",
        "### 🔧 Что нужно для Llama-3.2-1B:\n",
        "- **GPU память**: минимум 8GB (лучше 16GB)\n",
        "- **Время загрузки**: 2-5 минут (зависит от диска)\n",
        "- **Интернет**: Только для первого скачивания\n",
        "- **Права доступа**: Нужен HuggingFace токен для некоторых моделей\n",
        "\n",
        "\n",
        "#### Команда для загрузки модели\n",
        "Подробнее [в карточке модели](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
        "```python\n",
        "!huggingface-cli download \"meta-llama/Llama-3.2-1B-Instruct\" --local-dir \"./models/Llama-3.2-1B-Instruct\" --local-dir-use-symlinks False\n",
        "```\n",
        "\n",
        "__Важно__: убедитесь, что вы получили доступ к репозиторию модели, а так же сделали login через токен на вашем компьютере командой\n",
        "```bash\n",
        "huggingface-cli login\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('hf_token')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tl9AA83DRi5",
        "outputId": "22569267-f069-4133-8cc7-a359d580f46b"
      },
      "id": "3Tl9AA83DRi5",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6e49e6a2-368b-4ccc-b1bf-b220660d9392",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "6e49e6a2-368b-4ccc-b1bf-b220660d9392",
        "outputId": "f75d6f6b-d85f-4d15-e17c-5ccacca6e604"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    477\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3443960932.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./models/Llama-3.2-1B-Instruct\"\u001b[0m  \u001b[0;31m# Наша модель лежит по этому пути\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = AutoModelForCausalLM.from_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    880\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \"\"\"\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         resolved_files = [\n\u001b[0m\u001b[1;32m    529\u001b[0m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         resolved_files = [\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m ):\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed."
          ]
        }
      ],
      "source": [
        "model_id = \"./models/Llama-3.2-1B-Instruct\"  # Наша модель лежит по этому пути\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85e105a-a742-45b6-802b-650fe9bffc07",
      "metadata": {
        "id": "b85e105a-a742-45b6-802b-650fe9bffc07"
      },
      "outputs": [],
      "source": [
        "# У некоторых моделей (особенно fine-tuned для инструкций/чатов) нет токена для паддинга (`pad_token`)\n",
        "# Паддинг нужен для выравнивания длины последовательностей при пакетной обработке (batch processing)\n",
        "# Без этого токена возникнет ошибка\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e95e07c-0118-4e58-ad03-925d7f702e28",
      "metadata": {
        "id": "8e95e07c-0118-4e58-ad03-925d7f702e28",
        "outputId": "b1722201-e0ef-4b99-959f-9280a8a10b09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 5.66 s\n",
            "Wall time: 5.73 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"'What is the day today? \"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "input_token_count = inputs['input_ids'].shape[1]\n",
        "output_sequences = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.1,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "total_token_count = output_sequences.shape[1]\n",
        "output_token_count = total_token_count - input_token_count\n",
        "\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4958ab-89a2-4510-b7b5-a9813df39cec",
      "metadata": {
        "id": "6a4958ab-89a2-4510-b7b5-a9813df39cec",
        "outputId": "7db7e630-f033-47f3-bc13-0a09385efbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'What is the day today?  What is the day of the week?  What is the day of the month?  What is the day of the year?  What is the day of the week of the month?  What is the day of the month of the year?  What is the day of the year of the month?  What is the day of the week of the month of the year?  What is the day of the month of the year of the week of the month?  What is the day of the month of the year of the week of the month of the year?  What is the day of the week of the month of the year of the month of the week of the month of the year?  What is the day of the month of the year of the week of the month of the month of the week of the month of the year?  What is the day of the week of the month of the month of the year of the week of the month of the month of the week of the month of the year?  What is the day of the month of the year of the week of the month of the month of the week of the month of the year of the month of the week of the month of the year?  What is the day\n",
            "9 256 265\n"
          ]
        }
      ],
      "source": [
        "print(generated_text)\n",
        "print(input_token_count, output_token_count, total_token_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54532251-9f9d-41d5-9abc-00c59773b635",
      "metadata": {
        "id": "54532251-9f9d-41d5-9abc-00c59773b635"
      },
      "outputs": [],
      "source": [
        "# Все работает"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751320c1-c7cf-4d1f-a2a0-6bd276003e39",
      "metadata": {
        "id": "751320c1-c7cf-4d1f-a2a0-6bd276003e39"
      },
      "source": [
        "## 💡 \"Сейчас сделаю свой ChatGPT!\"\n",
        "\n",
        "### Типичная мысль разработчика:\n",
        "> \"У меня есть Transformers, есть FastAPI, есть GPU - значит, через пару часов у меня будет готовый продукт, который составит конкуренцию OpenAI!\"\n",
        "\n",
        "\n",
        "**Что кажется простым:**\n",
        "```python\n",
        "# Загрузил модель\n",
        "model = AutoModelForCausalLM.from_pretrained(\"llama-3.2-1b\")\n",
        "# Обернул в FastAPI\n",
        "@app.post(\"/generate\")\n",
        "def generate(text: str):\n",
        "    return model.generate(text)\n",
        "# Profit! 💰💰💰\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d102c3-a1ad-4129-aff0-dc03dd407cc0",
      "metadata": {
        "id": "22d102c3-a1ad-4129-aff0-dc03dd407cc0"
      },
      "outputs": [],
      "source": [
        "# мы использовали скрипт <fast_api_llm.py> для поднятия своего fast api сервиса с llm под капотом\n",
        "# давайте протестируем этот llm сервис"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27ae20d-10c3-4575-bf47-98e094dcad8d",
      "metadata": {
        "id": "a27ae20d-10c3-4575-bf47-98e094dcad8d",
        "outputId": "3dd2e968-46ce-4328-9c20-5456e6158714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 7.62 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = requests.post(\n",
        "    'http://localhost:8800/generate',\n",
        "    json={\"text\": 'What is the day today? '}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab4f4e8-cbbd-48dc-a7e3-c3ad79753859",
      "metadata": {
        "id": "2ab4f4e8-cbbd-48dc-a7e3-c3ad79753859",
        "outputId": "388c1ceb-dd94-42a0-aeda-91e1054909fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'generated_text': 'What is the day today?  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it is today.  I am trying to figure out what day it',\n",
              " 'input_token_count': 8,\n",
              " 'output_token_count': 256,\n",
              " 'total_token_count': 264,\n",
              " 'original_prompt': 'What is the day today? '}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3571367-8cfa-4789-b930-1d91b831276f",
      "metadata": {
        "id": "a3571367-8cfa-4789-b930-1d91b831276f"
      },
      "outputs": [],
      "source": [
        "# Отлично!\n",
        "# Какие метрики обычно снимают с API-сервисов? Давайте снимем их\n",
        "# для этого воспользуемся специально написанным скриптом <api_metrics_test.py>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26f51675-96c0-447a-a6ea-f65ce05d37e9",
      "metadata": {
        "id": "26f51675-96c0-447a-a6ea-f65ce05d37e9",
        "outputId": "b51e0be2-c34f-4ab3-c9e4-d4b48341f1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 API Metrics Testing Tool\n",
            "========================================\n",
            "Target URL: http://localhost:8800\n",
            "Sequential requests: 5\n",
            "Concurrent requests: 10 (workers: 3) \n",
            "Stress test: 15s (workers: 3) \n",
            "\n",
            "✅ API health check passed!\n",
            "\n",
            "🔄 Running 5 sequential requests...\n",
            "Request 1/5: ✅ (7.77s)\n",
            "Request 2/5: ✅ (6.13s)\n",
            "Request 3/5: ✅ (7.30s)\n",
            "Request 4/5: ✅ (7.44s)\n",
            "Request 5/5: ✅ (7.47s)\n",
            "\n",
            "🚀 Running 10 concurrent requests with 3 workers...\n",
            "Request 1/10: ✅ (7.54s)\n",
            "Request 2/10: ✅ (13.12s)\n",
            "Request 3/10: ✅ (16.99s)\n",
            "Request 4/10: ✅ (9.41s)\n",
            "Request 5/10: ✅ (20.58s)\n",
            "Request 6/10: ✅ (11.11s)\n",
            "Request 7/10: ✅ (19.06s)\n",
            "Request 8/10: ✅ (7.96s)\n",
            "Request 9/10: ✅ (19.01s)\n",
            "Request 10/10: ✅ (16.54s)\n",
            "\n",
            "💪 Running stress test for 15 seconds with 3 workers...\n",
            "\n",
            "============================================================\n",
            "📊 API PERFORMANCE METRICS REPORT\n",
            "============================================================\n",
            "Timestamp: 2025-07-20 14:34:59\n",
            "\n",
            "📈 Test Summary:\n",
            "  Total Requests: 165\n",
            "  Successful: 165 (100.0%)\n",
            "  Failed: 0 (0.0%)\n",
            "\n",
            "⏱️  Response Time Metrics:\n",
            "  Average: 3170.13 ms\n",
            "  Median: 2298.26 ms\n",
            "  Min: 2253.39 ms\n",
            "  Max: 20582.49 ms\n",
            "  95th percentile: 7964.83 ms\n",
            "  99th percentile: 19056.56 ms\n",
            "\n",
            "🚀 Throughput Metrics:\n",
            "  Requests per second: 0.32\n",
            "  Average response time: 3.17 seconds\n",
            "\n",
            "🔤 Token Metrics:\n",
            "  Average input tokens: 6.33\n",
            "  Average output tokens: 30.52\n",
            "  Average total tokens: 36.85\n",
            "  Average tokens/second: 8.66\n",
            "  Max tokens/second: 35.74\n",
            "\n",
            "💾 Metrics saved to: api_metrics_20250720_143459.json\n"
          ]
        }
      ],
      "source": [
        "%run src/api_metrics_test.py --base-url http://localhost:8800"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d83a7a-adcd-432d-9795-23833db35453",
      "metadata": {
        "id": "d3d83a7a-adcd-432d-9795-23833db35453"
      },
      "outputs": [],
      "source": [
        "# и что же получили?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a2a462-4aa2-46dc-b67f-f7c7b5d9d3aa",
      "metadata": {
        "id": "09a2a462-4aa2-46dc-b67f-f7c7b5d9d3aa"
      },
      "source": [
        "### 📝 что видим\n",
        "\n",
        "1.  **Проблема \"длинного хвоста\" (Tail Latency):** Существует значительный разрыв между медианным (`2.3 сек`) и средним (`3.17 сек`) временем ответа. Это указывает на наличие небольшого количества очень медленных запросов, которые сильно искажают общую картину.\n",
        "    *   **Критический показатель:** Максимальное время ответа в `20.6 секунд` и 95-й перцентиль в `~8 секунд` являются недопустимо высокими для большинства интерактивных приложений. Это означает, что каждый 20-й пользователь ждет ответа 8 секунд или дольше\n",
        "\n",
        "2.  **Низкая нагрузка теста:** Пропускная способность в `0.32 RPS` (примерно 1 запрос каждые 3 секунды) является очень низкой"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb523f1c-a573-48aa-87f2-9513d33dda84",
      "metadata": {
        "id": "eb523f1c-a573-48aa-87f2-9513d33dda84"
      },
      "source": [
        "## 🚨 Реальность FastAPI + Transformers: почему это не работает\n",
        "\n",
        "### Проблемы \"наивного\" подхода:\n",
        "\n",
        "**1. Управление памятью:**\n",
        "- Каждый запрос создает новый KV-cache\n",
        "- Модель не освобождает GPU память между запросами\n",
        "- При 2-3 одновременных запросах = OUT_OF_MEMORY\n",
        "\n",
        "**2. Batching отсутствует:**\n",
        "- Каждый запрос обрабатывается отдельно\n",
        "- GPU используется на 10-20% от возможностей\n",
        "- Ужасная экономическая эффективность\n",
        "\n",
        "**3. Нет очередей и приоритизации:**\n",
        "- Длинные запросы блокируют короткие\n",
        "- Нет возможности отменить генерацию\n",
        "- Нет контроля нагрузки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb66ce35-97b0-4b73-9ff1-2f66a665057a",
      "metadata": {
        "id": "eb66ce35-97b0-4b73-9ff1-2f66a665057a"
      },
      "outputs": [],
      "source": [
        "# как нам понять, что такого решения недостаточно? Давайте снимем дополнительные метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f34046-01c6-423f-9fa9-23ae402a6476",
      "metadata": {
        "id": "a6f34046-01c6-423f-9fa9-23ae402a6476"
      },
      "source": [
        "## 📈 Ключевые метрики LLM в продакшене\n",
        "\n",
        "### Что измеряем и почему это важно?\n",
        "\n",
        "**⚡ Time to First Token (TTFT)**\n",
        "- **Что:** Время от запроса до первого токена ответа\n",
        "- **Важность:** Определяет \"отзывчивость\" для пользователя\n",
        "- **Хорошо:** < 200ms, **Плохо:** > 1000ms\n",
        "- **Влияет на:** UX (User Experience) в чатах и интерактивных приложениях\n",
        "\n",
        "**🚀 Tokens per Second (TPS)**  \n",
        "- **Что:** Скорость генерации токенов после первого\n",
        "- **Важность:** Определяет, как быстро \"печатается\" ответ\n",
        "- **Хорошо:** > 50 TPS, **Плохо:** < 10 TPS\n",
        "- **Влияет на:** Восприятие \"умности\" системы\n",
        "\n",
        "**⏰ Inter-Token Latency (ITL)**\n",
        "- **Что:** Задержка между соседними токенами\n",
        "- **Важность:** Должна быть стабильной для плавного вывода\n",
        "- Хорошо: < 50ms, Плохо: > 200ms\n",
        "- **Влияет на:** Плавность streaming-ответов\n",
        "\n",
        "**📊 System Throughput**\n",
        "- **Что:** Общее количество токенов/запросов в секунду\n",
        "- **Важность:** Определяет масштабируемость системы\n",
        "- **Измеряется:** RPS (requests/sec), Tokens/sec\n",
        "- **Влияет на:** Стоимость инфраструктуры и возможности роста\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a78803-db20-4b3b-9dc4-21084b7dae95",
      "metadata": {
        "id": "33a78803-db20-4b3b-9dc4-21084b7dae95"
      },
      "outputs": [],
      "source": [
        "# снимаем эти метрики с нашего сервиса\n",
        "# для этого воспользуемся еще одним написанным скриптом <llm_inference_metrics.py>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472d2137-424e-476f-97c4-8838e813ab98",
      "metadata": {
        "id": "472d2137-424e-476f-97c4-8838e813ab98",
        "outputId": "753d7a07-1629-405b-f185-c8c3ff9984d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 LLM Inference Metrics Testing Tool\n",
            "========================================\n",
            "📡 Target: http://localhost:8800/generate\n",
            "🔧 API Format: fastapi\n",
            "📊 Total requests: ~127\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🧠 LLM INFERENCE PERFORMANCE METRICS REPORT\n",
            "======================================================================\n",
            "Timestamp: 2025-07-20 17:34:33\n",
            "\n",
            "📊 Test Summary:\n",
            "  Total Requests: 127\n",
            "  Successful: 127 (100.0%)\n",
            "  Failed: 0\n",
            "\n",
            "⚡ Time to First Token (TTFT):\n",
            "  Average: 674.55 ms\n",
            "  Median: 684.32 ms\n",
            "  Min: 331.23 ms\n",
            "  Max: 1098.91 ms\n",
            "  95th percentile: 1069.69 ms\n",
            "  Std deviation: 214.13 ms\n",
            "\n",
            "🚀 Tokens per Second (TPS):\n",
            "  Average: 7.16 tokens/sec\n",
            "  Median: 5.1 tokens/sec\n",
            "  Min: 3.0 tokens/sec\n",
            "  Max: 24.08 tokens/sec\n",
            "  95th percentile: 23.34 tokens/sec\n",
            "  Std deviation: 5.27 tokens/sec\n",
            "\n",
            "⏰ Inter-Token Latency (ITL):\n",
            "  Average: 172.39 ms\n",
            "  Median: 179.1 ms\n",
            "  Min: 36.75 ms\n",
            "  Max: 306.56 ms\n",
            "  95th percentile: 297.49 ms\n",
            "  Std deviation: 73.72 ms\n",
            "\n",
            "📈 System Throughput:\n",
            "  Input tokens/second: 1.41\n",
            "  Output tokens/second: 6.04\n",
            "  Requests/second: 0.17\n",
            "  Total processing time: 729.68 seconds\n",
            "\n",
            "🔤 Token Efficiency:\n",
            "  Average input tokens: 8.08\n",
            "  Average output tokens: 34.69\n",
            "  Generation ratio: 4.31\n",
            "\n",
            "==================================================\n",
            "📋 SUMMARY TABLE\n",
            "==================================================\n",
            "Success Rate - 100.0%\n",
            "Total Requests - 127\n",
            "Average TTFT - 674.55 ms\n",
            "Median TTFT - 684.32 ms\n",
            "95th Percentile TTFT - 1069.69 ms\n",
            "Average TPS - 7.16 tokens/sec\n",
            "Median TPS - 5.1 tokens/sec\n",
            "Max TPS - 24.08 tokens/sec\n",
            "Average ITL - 172.39 ms\n",
            "Median ITL - 179.1 ms\n",
            "95th Percentile ITL - 297.49 ms\n",
            "Output Tokens/Second - 6.04\n",
            "Requests/Second - 0.17\n",
            "Average Input Tokens - 8.08\n",
            "Average Output Tokens - 34.69\n",
            "Generation Ratio - 4.31\n",
            "==================================================\n",
            "\n",
            "💾 LLM metrics saved to: llm_inference_metrics_20250720_173433.json\n",
            "CPU times: total: 344 ms\n",
            "Wall time: 2min 34s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%run src/llm_inference_metrics.py --base-url http://localhost:8800 --endpoint /generate --api-format fastapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928b33d3-8233-4ada-befe-62e2b8956a53",
      "metadata": {
        "id": "928b33d3-8233-4ada-befe-62e2b8956a53"
      },
      "outputs": [],
      "source": [
        "# делаем акцент на следующие полученные метрики\n",
        "#\n",
        "# Average TTFT - 674.55 ms\n",
        "# 95th Percentile TTFT - 1069.69 ms\n",
        "# Average TPS - 7.16 tokens/sec\n",
        "# Average ITL - 172.39 ms\n",
        "# 95th Percentile ITL - 297.49 ms\n",
        "# Output Tokens/Second - 6.04\n",
        "# Requests/Second - 0.17"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4f5718-4090-4b59-a23a-b410ce2543fa",
      "metadata": {
        "id": "8a4f5718-4090-4b59-a23a-b410ce2543fa"
      },
      "source": [
        "## 🎯 Сравнение с индустриальными стандартами\n",
        "\n",
        "На [этом сайте](https://artificialanalysis.ai/models#latency) можно посмотреть разные бенчмарки компаний\n",
        "\n",
        "### Что показывают наши метрики vs лидеры рынка?\n",
        "Все метрики на примере модели GPT-4.1\n",
        "\n",
        "\n",
        "**Наши результаты (FastAPI + Transformers):**\n",
        "- Average TTFT: ~675ms ⚠️\n",
        "- Average TPS: ~7 tokens/sec 🐌  \n",
        "- Requests/Second: ~0.17 💀\n",
        "\n",
        "**Лидеры индустрии:**\n",
        "- TTFT: ~500ms ⚡\n",
        "- TPS: ~145 tokens/sec 🚀\n",
        "- Concurrent users: тысячи одновременно 🏭\n",
        "\n",
        "### 📊 Насколько мы отстаем?\n",
        "\n",
        "| Метрика | Наш результат | Индустрия | Отставание |\n",
        "|---------|---------------|-----------|------------|\n",
        "| TPS | 7 | 145 | **20x хуже** |\n",
        "| TTFT | 675ms | 500ms | **1.35x хуже** |\n",
        "| RPS | 0.17 | 10+ | **60x хуже** |\n",
        "| Concurrent users | 1 | 1000+ | **1000x хуже** |\n",
        "\n",
        "### 💡 Вывод:\n",
        "**Сделать стартап за $40млрд с такими метриками не получится**\n",
        "\n",
        "\n",
        "Большинство разработчиков думают, что развернуть LLM в проде - это просто обернуть модель в API. Но реальность гораздо сложнее!\n",
        "\n",
        "Переход от research prototype к production system требует глубокого понимания:\n",
        "- Computer Systems Architecture\n",
        "- Distributed Computing Theory  \n",
        "- GPU Programming Models\n",
        "- Economic Optimization\n",
        "- Reliability Engineering\n",
        "\n",
        "Нужны кардинально другие подходы:\n",
        "- Специализированные inference движки\n",
        "- Продвинутое управление памятью  \n",
        "- Continuous batching\n",
        "- Оптимизированные CUDA ядра\n",
        "\n",
        "\n",
        "## 🎯 Проблематика Production LLM: Четыре ключевых вызова\n",
        "\n",
        "Просто \"запустить модель на сервере\" в мире LLM — это провал. В отличие от классических ML-моделей, инференс LLM создает уникальные и сложные инженерные проблемы.\n",
        "\n",
        "### 1. **Латентность (Latency)**\n",
        "\n",
        "Пользователи ожидают интерактивности, схожей с человеческим общением. В контексте LLM это разделяется на два показателя:\n",
        "\n",
        "- **Time to First Token (TTFT):** Время до получения первого токена. Должно быть минимальным (< 200-500ms), чтобы пользователь видел, что система \"начала отвечать\".\n",
        "- **Time per Output Token (TPOT):** Время генерации каждого последующего токена. Определяет \"скорость печати\" модели.\n",
        "\n",
        "\n",
        "### 2. **Стоимость и утилизация GPU (Cost \\& Utilization)**\n",
        "\n",
        "GPU-ускорители, необходимые для работы LLM, дороги (2-15 $ в час за один GPU). Чтобы сервис был экономически выгодным, каждый доллар, вложенный в оборудование, должен приносить максимальную отдачу. **Низкая утилизация GPU — это деньги, сжигаемые впустую.**\n",
        "\n",
        "### 3. **Масштабируемость (Scalability)**\n",
        "\n",
        "Сервис должен одинаково хорошо работать как для одного пользователя, так и для миллионов одновременных запросов. Нагрузка на LLM-сервисы непредсказуема и может взлетать в пиковые моменты.\n",
        "\n",
        "### 4. **Надежность (Reliability)**\n",
        "\n",
        "Основная причина сбоев в LLM-сервисах — ошибки нехватки памяти (Out of Memory, OOM). Длинные запросы или всплеск трафика могут легко исчерпать память GPU, приводя к отказам в обслуживании. Цель — обеспечить доступность сервиса на уровне 99.9% и выше.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77fc0ab2-dbbc-4fef-90b0-9ea58fba1316",
      "metadata": {
        "id": "77fc0ab2-dbbc-4fef-90b0-9ea58fba1316"
      },
      "source": [
        "____\n",
        "# Часть 2\n",
        "Здесь мы разберем следующие темы\n",
        "- 🏗️ Архитектурные паттерны инференса\n",
        "- 🚄 Сравнение vLLM и TensorRT-LLM\n",
        "## 🔧 Архитектурные паттерны инференса\n",
        "\n",
        "В основе производительности LLM лежат два различных режима работы GPU: обработка промпта и генерация ответа. Понимание их различий — ключ к оптимизации.\n",
        "\n",
        "### Фазы вычислений: Prefill vs. Decode\n",
        "\n",
        "#### **Prefill-фаза**\n",
        "\n",
        "Это первый этап, когда модель обрабатывает входной промпт пользователя.\n",
        "\n",
        "* **Процесс:** Все токены промпта (`\"Объясни квантовую физику простыми словами\"`) обрабатываются **одновременно и параллельно**.\n",
        "* **Характеристики:**\n",
        "    * **Высокая загрузка вычислительных ядер GPU (Compute-bound):** На этом этапе выполняются массивные матричные умножения.\n",
        "    * **Высокий параллелизм:** Идеальный сценарий для архитектуры GPU.\n",
        "    * **Результат:** Расчет внутреннего состояния (KV-кэша) для всего промпта и генерация **первого** токена ответа.\n",
        "\n",
        "\n",
        "#### **Decode-фаза**\n",
        "\n",
        "Это второй этап, когда модель генерирует ответ токен за токеном.\n",
        "\n",
        "* **Процесс:** Генерация носит авторегрессионный характер: чтобы сгенерировать токен `N`, нужно знать токен `N-1`. Это строго **последовательная** операция.\n",
        "* **Характеристики:**\n",
        "    * **Низкая загрузка вычислительных ядер GPU:** На каждом шаге выполняется лишь небольшая работа.\n",
        "    * **Ограничение пропускной способностью памяти (Memory-bandwidth bound):** Скорость определяется тем, как быстро GPU может читать и записывать KV-кэш из своей памяти.\n",
        "    * **Низкая утилизация GPU:** Большую часть времени дорогие вычислительные ядра простаивают.\n",
        "\n",
        "> **Ключевой вывод:** Фаза **Prefill** эффективно использует GPU, в то время как фаза **Decode** — крайне неэффективно. Именно оптимизация медленной Decode-фазы приносит наибольший выигрыш в производительности."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34491df-1e45-47ca-967a-b32235fed111",
      "metadata": {
        "id": "f34491df-1e45-47ca-967a-b32235fed111"
      },
      "outputs": [],
      "source": [
        "# подробнее - некоторые важные особенности\n",
        "# https://www.bentoml.com/llm/inference-optimization/prefill-decode-disaggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7463819-afde-4b6c-9f24-9d77d6f4d801",
      "metadata": {
        "id": "b7463819-afde-4b6c-9f24-9d77d6f4d801"
      },
      "outputs": [],
      "source": [
        "# как решить эту проблему?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec952856-2a32-48f5-aec7-ff373f72b370",
      "metadata": {
        "id": "ec952856-2a32-48f5-aec7-ff373f72b370"
      },
      "source": [
        "### KV Cache: Память, которая всё меняет (и съедает)\n",
        "\n",
        "**Что это?**\n",
        "KV-кэш — это \"краткосрочная память\" модели для текущего запроса. В механизме внимания (attention) модели необходимо иметь доступ к информации о всех предыдущих токенах в последовательности. KV-кэш хранит предварительно вычисленные векторы ключей (Keys) и значений (Values) для каждого токена. Это избавляет от необходимости пересчитывать их заново при генерации каждого нового токена.\n",
        "\n",
        "**Проблема:**\n",
        "\n",
        "\n",
        "Размер KV-кэша растет линейно с длиной последовательности и является главным потребителем видеопамяти (VRAM).\n",
        "\n",
        "```python\n",
        "# Формула для расчета размера KV-кэша на один запрос\n",
        "# (для float16, где каждый параметр занимает 2 байта)\n",
        "\n",
        "kv_cache_size_bytes = (\n",
        "      num_layers * 2  # K и V\n",
        "    * hidden_size\n",
        "    * sequence_length\n",
        "    * batch_size\n",
        "    * 2  # байт на элемент (float16)\n",
        ")\n",
        "\n",
        "# Пример для Llama-3-8B с длиной контекста 4096 токенов:\n",
        "# 32 layers * 2 * 4096 hidden_size * 4096 tokens * 1 batch * 2 bytes = ~2.1 GB\n",
        "```\n",
        "\n",
        "**Почему это критично:**\n",
        "\n",
        "1. **Ограничивает Batch Size:** Чем больше памяти съедает один запрос, тем меньше одновременных запросов может обработать GPU.\n",
        "2. **Ограничивает максимальный контекст:** Доступная VRAM напрямую определяет, какой длины промпты и ответы вы можете поддерживать.\n",
        "3. **Главная причина OOM-ошибок:** Большой входящий промпт или пик трафика мгновенно исчерпывают память.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206fbf89-2aa7-46a3-9506-1ede44a3589b",
      "metadata": {
        "id": "206fbf89-2aa7-46a3-9506-1ede44a3589b"
      },
      "outputs": [],
      "source": [
        "# а еще проблемы будут?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ee510f-5978-48cd-911c-c849651cd458",
      "metadata": {
        "id": "85ee510f-5978-48cd-911c-c849651cd458"
      },
      "source": [
        "\n",
        "### Проблема: Статический батчинг (Static Batching)\n",
        "\n",
        "Традиционный подход к батчингу, унаследованный из обучения моделей.\n",
        "\n",
        "* **Как работает:** Несколько запросов объединяются в один батч. Модель обрабатывает батч целиком.\n",
        "* **Главный недостаток:** Все последовательности в батче должны иметь одинаковую длину. Короткие запросы дополняются \"пустыми\" токенами (padding) до длины самого длинного запроса.\n",
        "* **Результат:** GPU тратит драгоценные циклы на обработку бесполезных padding-токенов. Более того, весь батч не завершится, пока не будет сгенерирован ответ на самый длинный запрос.\n",
        "\n",
        "> **Аналогия:** Автобус, который ждет, пока *абсолютно все* его пассажиры доберутся до своих конечных остановок, прежде чем сможет взять новых, даже если половина мест освободилась в самом начале маршрута."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f91d03-ee82-49ad-8886-5da2d6692044",
      "metadata": {
        "id": "b7f91d03-ee82-49ad-8886-5da2d6692044"
      },
      "outputs": [],
      "source": [
        "# как решить эту проблему?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b95a56d-ab3c-4fbf-bd41-1ad473256f1a",
      "metadata": {
        "id": "9b95a56d-ab3c-4fbf-bd41-1ad473256f1a"
      },
      "source": [
        "### Решение: Непрерывный батчинг (Continuous Batching)\n",
        "\n",
        "Этот подход, популяризированный фреймворком **vLLM**, вносит кардинальные изменения\n",
        "\n",
        "* **Как работает:** Вместо батчинга на уровне всего батча, управление происходит на уровне отдельных запросов.\n",
        "\n",
        "1. Как только генерация для одного запроса в батче завершается, он немедленно удаляется.\n",
        "2. В освободившееся в памяти GPU место тут же помещается новый ожидающий запрос.\n",
        "3. Таким образом, батч постоянно \"пересобирается\", а GPU никогда не простаивает и не тратит время на padding.\n",
        "* **Ключевой механизм:** Для реализации этого используется техника **PagedAttention**, которая управляет KV-кэшем аналогично тому, как операционная система управляет виртуальной памятью — делит память на \"страницы\" и динамически выделяет их запросам. Это решает проблему фрагментации памяти.\n",
        "\n",
        "> **Аналогия:** Ресторан с эффективным хостес. Как только столик освобождается, за него сразу сажают следующую компанию из очереди. Ни одно место не простаивает.\n",
        "\n",
        "**Результат:** **Увеличение пропускной способности (throughput) в 2–20 раз** по сравнению со статическим батчингом. Это одна из самых важных оптимизаций для современных LLM-сервисов.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98580f96-4144-45fb-a4b4-38cd9e93d90d",
      "metadata": {
        "id": "98580f96-4144-45fb-a4b4-38cd9e93d90d"
      },
      "outputs": [],
      "source": [
        "# не самим же все это писать?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e373f46-955e-412c-a664-5c76aae7aa8d",
      "metadata": {
        "id": "5e373f46-955e-412c-a664-5c76aae7aa8d"
      },
      "source": [
        "## Обзор ключевых фреймворков: vLLM и TensorRT-LLM\n",
        "\n",
        "Среди множества движков для инференса выделяются два основных решения для высокопроизводительных вычислений на GPU, каждое со своей специализацией.\n",
        "\n",
        "### **vLLM: Фокус на управлении памятью и пропускной способности**\n",
        "\n",
        "**vLLM** — это [библиотека с открытым исходным](https://github.com/vllm-project/vllm) кодом от UC Berkeley, которая предложила подход, значительно повышающий эффективность инференса.\n",
        "\n",
        "*   **Основной принцип:** Ключевая проблема при обслуживании LLM-запросов заключается в неэффективном управлении памятью, выделяемой под KV-кэш. Оптимизация этого аспекта напрямую ведет к росту пропускной способности.\n",
        "*   **Ключевая технология: PagedAttention**\n",
        "    *   По аналогии с системами виртуальной памяти в ОС, PagedAttention организует KV-кэш в виде не-непрерывных блоков (страниц) памяти.\n",
        "    *   Это устраняет проблему внешней фрагментации памяти, из-за которой значительная часть VRAM могла оставаться незадействованной.\n",
        "    *   В результате достигается утилизация памяти с эффективностью выше 96%.\n",
        "*   **Основная функциональность: Continuous Batching**\n",
        "    *   Эффективное управление памятью через PagedAttention является основой для реализации непрерывного батчинга.\n",
        "    *   Система динамически управляет составом батча, добавляя новые запросы по мере освобождения ресурсов, что обеспечивает постоянную и высокую загрузку GPU\n",
        "\n",
        "[подробнее тут](https://www.ubicloud.com/blog/life-of-an-inference-request-vllm-v1)\n",
        "\n",
        "### **TensorRT-LLM: Фокус на компиляции и низкоуровневых оптимизациях**\n",
        "\n",
        "**TensorRT-LLM** — это [библиотека от NVIDIA](https://github.com/NVIDIA/TensorRT-LLM), предназначенная для достижения пиковой производительности LLM на графических процессорах компании.\n",
        "\n",
        "*   **Основной принцип:** Обеспечение максимальной производительности за счет компиляции модели в высокооптимизированный исполняемый код, адаптированный под конкретную архитектуру GPU (например, Hopper, Ampere) и параметры задачи.\n",
        "*   **Ключевая технология: Компиляция и CUDA-оптимизации**\n",
        "    *   **Kernel Fusion (Слияние ядер):** Несколько последовательных операций CUDA объединяются в одно оптимизированное ядро, что значительно сокращает накладные расходы на вызовы и обращения к глобальной памяти GPU.\n",
        "    *   **Графовые оптимизации:** Анализ и преобразование вычислительного графа модели для более эффективного исполнения.\n",
        "    *   **Поддержка числовых форматов:** Использование форматов пониженной точности, таких как FP8 на новейших архитектурах, для ускорения вычислений и снижения требований к памяти.\n",
        "*   **Основная функциональность:** TensorRT-LLM интегрирует собственные реализации PagedAttention и In-flight Batching, сочетая их с глубокими аппаратными оптимизациями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e881598d-d50a-4cc1-926f-34812495f878",
      "metadata": {
        "id": "e881598d-d50a-4cc1-926f-34812495f878"
      },
      "outputs": [],
      "source": [
        "# значит будем сравнивать vLLM и TensorRT - с использованием контейнера triton от nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184c6406-e9c5-46e9-a006-ceb8481596be",
      "metadata": {
        "id": "184c6406-e9c5-46e9-a006-ceb8481596be"
      },
      "outputs": [],
      "source": [
        "# начнем с vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd2322d-f647-41f6-b826-12756de0f5c4",
      "metadata": {
        "id": "bcd2322d-f647-41f6-b826-12756de0f5c4"
      },
      "outputs": [],
      "source": [
        "# у нас есть запущенный контейнер, подробная инструкция есть в папке triton_instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491bfda3-4fd7-45e3-8e71-9dd2bc503d40",
      "metadata": {
        "id": "491bfda3-4fd7-45e3-8e71-9dd2bc503d40"
      },
      "outputs": [],
      "source": [
        "# пробуем постучаться в контейнер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660a9ee0-e107-4c4e-a5a8-08aa4a76b87e",
      "metadata": {
        "id": "660a9ee0-e107-4c4e-a5a8-08aa4a76b87e",
        "outputId": "5700163b-1abc-4c79-fbeb-36fd5c504d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 385 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "payload = {\n",
        "    \"text_input\": 'What is the day today? ',\n",
        "    \"stream\": False,\n",
        "    \"max_tokens\": 15,\n",
        "    \"temperature\": 0.1\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000/v2/models/llama3_2_1b_local/generate\",\n",
        "    json=payload\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aaae1ac-3b94-4bf7-99cb-a73576e9525f",
      "metadata": {
        "id": "4aaae1ac-3b94-4bf7-99cb-a73576e9525f",
        "outputId": "c14ce11c-b981-4464-9a0b-8efb1bfcfeac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_name': 'llama3_2_1b_local',\n",
              " 'model_version': '1',\n",
              " 'text_output': 'What is the day today?  I am trying to figure out what day of the week it is today.'}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fcf494e-31b7-4d23-a2e6-4b43d9820457",
      "metadata": {
        "id": "0fcf494e-31b7-4d23-a2e6-4b43d9820457"
      },
      "outputs": [],
      "source": [
        "# работает!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9397dfbf-9f55-4a91-8b4a-bd8d8fe774d3",
      "metadata": {
        "id": "9397dfbf-9f55-4a91-8b4a-bd8d8fe774d3"
      },
      "outputs": [],
      "source": [
        "# собираем метрики, как для нашего fast api сервиса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797773f0-2266-4a59-93eb-1328646c5a2b",
      "metadata": {
        "id": "797773f0-2266-4a59-93eb-1328646c5a2b",
        "outputId": "a6866beb-2ac5-4cae-ba8c-2aaf2d0047aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 LLM Inference Metrics Testing Tool\n",
            "========================================\n",
            "📡 Target: http://localhost:8000/v2/models/llama3_2_1b_local/generate\n",
            "🔧 API Format: triton\n",
            "📊 Total requests: ~127\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🧠 LLM INFERENCE PERFORMANCE METRICS REPORT\n",
            "======================================================================\n",
            "Timestamp: 2025-07-20 17:43:24\n",
            "\n",
            "📊 Test Summary:\n",
            "  Total Requests: 127\n",
            "  Successful: 127 (100.0%)\n",
            "  Failed: 0\n",
            "\n",
            "⚡ Time to First Token (TTFT):\n",
            "  Average: 120.73 ms\n",
            "  Median: 115.78 ms\n",
            "  Min: 34.87 ms\n",
            "  Max: 210.93 ms\n",
            "  95th percentile: 208.38 ms\n",
            "  Std deviation: 29.29 ms\n",
            "\n",
            "🚀 Tokens per Second (TPS):\n",
            "  Average: 81.28 tokens/sec\n",
            "  Median: 81.6 tokens/sec\n",
            "  Min: 48.66 tokens/sec\n",
            "  Max: 102.39 tokens/sec\n",
            "  95th percentile: 95.41 tokens/sec\n",
            "  Std deviation: 8.13 tokens/sec\n",
            "\n",
            "⏰ Inter-Token Latency (ITL):\n",
            "  Average: 9.1 ms\n",
            "  Median: 8.93 ms\n",
            "  Min: 7.19 ms\n",
            "  Max: 15.04 ms\n",
            "  95th percentile: 10.55 ms\n",
            "  Std deviation: 0.98 ms\n",
            "\n",
            "📈 System Throughput:\n",
            "  Input tokens/second: 18.26\n",
            "  Output tokens/second: 82.44\n",
            "  Requests/second: 2.32\n",
            "  Total processing time: 54.66 seconds\n",
            "\n",
            "🔤 Token Efficiency:\n",
            "  Average input tokens: 7.86\n",
            "  Average output tokens: 35.48\n",
            "  Generation ratio: 4.79\n",
            "\n",
            "==================================================\n",
            "📋 SUMMARY TABLE\n",
            "==================================================\n",
            "Success Rate - 100.0%\n",
            "Total Requests - 127\n",
            "Average TTFT - 120.73 ms\n",
            "Median TTFT - 115.78 ms\n",
            "95th Percentile TTFT - 208.38 ms\n",
            "Average TPS - 81.28 tokens/sec\n",
            "Median TPS - 81.6 tokens/sec\n",
            "Max TPS - 102.39 tokens/sec\n",
            "Average ITL - 9.1 ms\n",
            "Median ITL - 8.93 ms\n",
            "95th Percentile ITL - 10.55 ms\n",
            "Output Tokens/Second - 82.44\n",
            "Requests/Second - 2.32\n",
            "Average Input Tokens - 7.86\n",
            "Average Output Tokens - 35.48\n",
            "Generation Ratio - 4.79\n",
            "==================================================\n",
            "\n",
            "💾 LLM metrics saved to: llm_inference_metrics_20250720_174324.json\n",
            "CPU times: total: 219 ms\n",
            "Wall time: 20 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%run src/llm_inference_metrics.py --base-url http://localhost:8000 --endpoint /v2/models/llama3_2_1b_local/generate --api-format triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd423816-9146-4001-a331-a3f7b6552eef",
      "metadata": {
        "id": "bd423816-9146-4001-a331-a3f7b6552eef"
      },
      "outputs": [],
      "source": [
        "# сравним"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123718c2-9937-45c1-a2b9-dc68f807b0a2",
      "metadata": {
        "id": "123718c2-9937-45c1-a2b9-dc68f807b0a2"
      },
      "source": [
        "## Сравнение показателей\n",
        "\n",
        "| Метрика                  | FastAPI                 | vLLM                  | Улучшение           |\n",
        "|--------------------------|------------------------|-----------------------|---------------------|\n",
        "| Average TTFT             | 674.55 ms              | 120.73 ms             | ~5.6x быстрее       |\n",
        "| 95th Percentile TTFT     | 1069.69 ms             | 208.38 ms             | ~5.1x быстрее       |\n",
        "| Average TPS              | 7.16 tokens/sec        | 81.28 tokens/sec      | ~11.4x выше         |\n",
        "| Average ITL              | 172.39 ms              | 9.1 ms                | ~19x быстрее        |\n",
        "| 95th Percentile ITL      | 297.49 ms              | 10.55 ms              | ~28x быстрее        |\n",
        "| Output Tokens/Second     | 6.04                   | 82.44                 | ~13.6x выше         |\n",
        "| Requests/Second          | 0.17                   | 2.32                  | ~13.6x выше         |\n",
        "\n",
        "- **Time to First Token (TTFT):** vLLM значительно уменьшает как средний, так и 95-й процентиль задержки первого токена.\n",
        "- **Token Per Second (TPS):** vLLM обеспечивает более чем 10-кратное увеличение скорости генерации.\n",
        "- **Output Tokens/Second и Requests/Second:** Рост пропускной способности при применении vLLM наблюдается почти на порядок."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1daf35d7-0d43-4cf9-8398-7f3f93096d02",
      "metadata": {
        "id": "1daf35d7-0d43-4cf9-8398-7f3f93096d02"
      },
      "outputs": [],
      "source": [
        "# теперь проверяем TensorRT-LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1481284a-07b8-4d57-85da-9c532b7ba887",
      "metadata": {
        "id": "1481284a-07b8-4d57-85da-9c532b7ba887"
      },
      "source": [
        "## Зачем нужна компиляция модели с помощью TensorRT-LLM?\n",
        "\n",
        "**Компиляция** с помощью TensorRT-LLM — это процесс преобразования стандартной, \"универсальной\" языковой модели (например, из Hugging Face) в сверхэффективный, специализированный **\"движок\" (engine)**. Этот движок создан для максимально быстрой работы на **конкретной модели GPU NVIDIA**\n",
        "\n",
        "Если вы планируете использовать LLM не для экспериментов, а в реальном продукте, этот шаг становится критически важным.\n",
        "\n",
        "### Ключевые цели оптимизации\n",
        "\n",
        "Компиляция решает три главные задачи, без которых развертывание LLM становится неэффективным и дорогим\n",
        "\n",
        "1.  **⬇️ Низкая задержка (Low Latency):** Значительно сокращается время генерации ответа. Это то, что напрямую влияет на пользовательский опыт.\n",
        "2.  **⬆️ Высокая пропускная способность (High Throughput):** Та же видеокарта может обслуживать гораздо больше запросов одновременно. Это напрямую снижает затраты на инфраструктуру\n",
        "3.  **🧠 Эффективное использование памяти:** Оптимизированная модель потребляет меньше видеопамяти (VRAM), что позволяет запускать более крупные модели на том же оборудовании\n",
        "\n",
        "Согласно тестам NVIDIA, прирост в скорости может достигать **8 раз** по сравнению с неоптимизированными версиями\n",
        "\n",
        "### Что происходит \"под капотом\" во время компиляции?\n",
        "\n",
        "Когда вы запускаете команду `trtllm-build`, TensorRT-LLM выполняет глубокую оптимизацию модели под ваше \"железо\". Основные техники включают:\n",
        "\n",
        "*   **Слияние слоев (Layer Fusion):** Вместо выполнения множества мелких операций (умножение матриц, сложение, функция активации) последовательно, TensorRT объединяет их в одно большое, оптимизированное ядро CUDA. Это резко сокращает обращения к памяти GPU.\n",
        "*   **Квантование (Quantization):** Веса и вычисления модели переводятся в форматы с пониженной точностью (например, **FP16**, **INT8**). Эти форматы обрабатываются значительно быстрее на современных Tensor Cores в GPU NVIDIA и требуют меньше памяти\n",
        "*   **Автоматический подбор ядер (Kernel Auto-Tuning):** TensorRT тестирует различные алгоритмы для выполнения операций на вашем GPU и выбирает самый быстрый из них\n",
        "*   **Продвинутое управление памятью:** Внедряются такие техники, как **Paged Attention** и **In-flight Batching**, которые оптимизируют использование K/V кэша, позволяя эффективно обрабатывать множество запросов разной длины одновременно\n",
        "\n",
        "### Практические выводы и компромиссы\n",
        "\n",
        "| Характеристика | Стандартная модель (Hugging Face) | Скомпилированная модель (TensorRT-LLM) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Скорость** | Стандартная | Максимально возможная для данного GPU |\n",
        "| **Пропускная способность** | Базовая | Значительно выше |\n",
        "| **Потребление VRAM** | Высокое | Сниженное |\n",
        "| **Гибкость** | **Универсальность:** работает на любом GPU | **Специализация:** привязана к конкретной модели GPU |\n",
        "\n",
        "> **❗Важно помнить:** Скомпилированный движок **не является переносимым**. Если вы захотите запустить модель на другой модели GPU или измените параметры самой модели (например, точность), вам придется **выполнить компиляцию заново**. Вы компилируете модель один раз для конкретной связки \"модель + параметры + GPU\" и затем многократно используете полученный движок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed09ae48-9802-4622-9cc9-f21c30977d07",
      "metadata": {
        "id": "ed09ae48-9802-4622-9cc9-f21c30977d07"
      },
      "outputs": [],
      "source": [
        "# имеем на руках развернутый контейнер - пробуем в него постучаться"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "178a3d22-0ef0-4165-b921-9cbe011f6bbd",
      "metadata": {
        "id": "178a3d22-0ef0-4165-b921-9cbe011f6bbd",
        "outputId": "49460e2f-68c0-47cc-9fb4-cd67ef8fed32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_name': 'ensemble', 'model_version': '1', 'parameters': {'sequence_id': 0, 'sequence_start': False, 'sequence_end': False}, 'outputs': [{'name': 'text_output', 'datatype': 'BYTES', 'shape': [1], 'data': [' Deep learning is a type of artificial intelligence (AI) that uses algorithms and statistical models to analyze and interpret complex data. It is a subset of machine learning, which is a broader field of study that involves the use of algorithms and statistical models to enable machines to perform tasks that typically require human intelligence.\\n\\nDeep learning is particularly useful for tasks that involve:\\n\\n1. Image and video analysis\\n2. Natural']}]}\n",
            "CPU times: total: 0 ns\n",
            "Wall time: 1.07 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "url = \"http://localhost:8000/v2/models/ensemble/infer\"\n",
        "data = {\n",
        "    \"inputs\": [\n",
        "        {\n",
        "            \"name\": \"text_input\",\n",
        "            \"shape\": [1, 1],\n",
        "            \"datatype\": \"BYTES\",\n",
        "            \"data\": [\"What is deep learning?\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"max_tokens\",\n",
        "            \"shape\": [1, 1],\n",
        "            \"datatype\": \"INT32\",\n",
        "            \"data\": [80]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e25825-6ab8-485b-bf9c-be926be77fbf",
      "metadata": {
        "id": "c8e25825-6ab8-485b-bf9c-be926be77fbf"
      },
      "outputs": [],
      "source": [
        "# собираем метрики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc5e33c-51e3-4596-95ca-48b96ba9c347",
      "metadata": {
        "id": "bcc5e33c-51e3-4596-95ca-48b96ba9c347",
        "outputId": "49368adf-e1f3-4e4d-f084-aa9dafb0758c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 LLM Inference Metrics Testing Tool\n",
            "========================================\n",
            "📡 Target: http://localhost:8000/v2/models/ensemble/infer\n",
            "🔧 API Format: tensorrt\n",
            "📊 Total requests: ~127\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🧠 LLM INFERENCE PERFORMANCE METRICS REPORT\n",
            "======================================================================\n",
            "Timestamp: 2025-07-20 17:39:56\n",
            "\n",
            "📊 Test Summary:\n",
            "  Total Requests: 127\n",
            "  Successful: 127 (100.0%)\n",
            "  Failed: 0\n",
            "\n",
            "⚡ Time to First Token (TTFT):\n",
            "  Average: 121.06 ms\n",
            "  Median: 106.74 ms\n",
            "  Min: 36.74 ms\n",
            "  Max: 205.88 ms\n",
            "  95th percentile: 203.91 ms\n",
            "  Std deviation: 35.32 ms\n",
            "\n",
            "🚀 Tokens per Second (TPS):\n",
            "  Average: 82.01 tokens/sec\n",
            "  Median: 87.67 tokens/sec\n",
            "  Min: 42.41 tokens/sec\n",
            "  Max: 105.4 tokens/sec\n",
            "  95th percentile: 99.89 tokens/sec\n",
            "  Std deviation: 16.59 tokens/sec\n",
            "\n",
            "⏰ Inter-Token Latency (ITL):\n",
            "  Average: 9.52 ms\n",
            "  Median: 8.6 ms\n",
            "  Min: 6.91 ms\n",
            "  Max: 18.19 ms\n",
            "  95th percentile: 15.45 ms\n",
            "  Std deviation: 2.72 ms\n",
            "\n",
            "📈 System Throughput:\n",
            "  Input tokens/second: 17.92\n",
            "  Output tokens/second: 80.05\n",
            "  Requests/second: 2.28\n",
            "  Total processing time: 55.7 seconds\n",
            "\n",
            "🔤 Token Efficiency:\n",
            "  Average input tokens: 7.86\n",
            "  Average output tokens: 35.11\n",
            "  Generation ratio: 4.76\n",
            "\n",
            "==================================================\n",
            "📋 SUMMARY TABLE\n",
            "==================================================\n",
            "Success Rate - 100.0%\n",
            "Total Requests - 127\n",
            "Average TTFT - 121.06 ms\n",
            "Median TTFT - 106.74 ms\n",
            "95th Percentile TTFT - 203.91 ms\n",
            "Average TPS - 82.01 tokens/sec\n",
            "Median TPS - 87.67 tokens/sec\n",
            "Max TPS - 105.4 tokens/sec\n",
            "Average ITL - 9.52 ms\n",
            "Median ITL - 8.6 ms\n",
            "95th Percentile ITL - 15.45 ms\n",
            "Output Tokens/Second - 80.05\n",
            "Requests/Second - 2.28\n",
            "Average Input Tokens - 7.86\n",
            "Average Output Tokens - 35.11\n",
            "Generation Ratio - 4.76\n",
            "==================================================\n",
            "\n",
            "💾 LLM metrics saved to: llm_inference_metrics_20250720_173956.json\n",
            "CPU times: total: 250 ms\n",
            "Wall time: 19.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%run src/llm_inference_metrics.py --base-url http://localhost:8000 --endpoint /v2/models/ensemble/infer --api-format tensorrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773c9499-6b10-4239-a707-406fbed077d5",
      "metadata": {
        "id": "773c9499-6b10-4239-a707-406fbed077d5"
      },
      "outputs": [],
      "source": [
        "# сравним"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e002bc30-62a3-4d59-967f-803de1d9f335",
      "metadata": {
        "id": "e002bc30-62a3-4d59-967f-803de1d9f335"
      },
      "source": [
        "## сравнение\n",
        "\n",
        "| Метрика                  | FastAPI (Базовый)      | vLLM (Оптимизированный) | TensorRT-LLM (Оптимизированный) |\n",
        "|--------------------------|------------------------|-------------------------|---------------------------------|\n",
        "| Average TTFT             | 674.55 ms              | 120.73 ms               | 121.06 ms                       |\n",
        "| 95th Percentile TTFT     | 1069.69 ms             | 208.38 ms               | 203.91 ms                       |\n",
        "| Average TPS              | 7.16 tokens/sec        | 81.28 tokens/sec        | 82.01 tokens/sec                |\n",
        "| Average ITL              | 172.39 ms              | 9.1 ms                  | 9.52 ms                         |\n",
        "| Requests/Second          | 0.17                   | 2.32                    | 2.28                            |\n",
        "\n",
        "Данные демонстрируют, что как vLLM, так и TensorRT-LLM показывают схожие, кардинальные улучшения по сравнению с базовым подходом:\n",
        "\n",
        "-   **Латентность (TTFT, ITL):** Снижение в 5-20 раз.\n",
        "-   **Пропускная способность (TPS, Requests/Second):** Увеличение более чем в 10 раз.\n",
        "\n",
        "Это подтверждает, что использование специализированных движков инференса является ключевым фактором для достижения высокой производительности"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e844a0c-b620-4cc5-88ae-2f55eec02050",
      "metadata": {
        "id": "7e844a0c-b620-4cc5-88ae-2f55eec02050"
      },
      "source": [
        "# Где и кто применяет TensorRT-LLM и vLLM\n",
        "\n",
        "| Компания / организация | Продукт или сервис | Движок инференса | Как используется | Ключевые эффекты |\n",
        "|------------------------|--------------------|------------------|------------------|------------------|\n",
        "| **[Amazon](https://pytorch.org/blog/vllm-joins-pytorch/)** | Shopping-ассистент Rufus | vLLM (на AWS Inferentia / Trainium) | Обслуживает миллионы запросов в мобильном приложении, выводя первый токен < 1 с даже в периоды Prime Day | 3 млн токенов/мин при P99 TTFT < 1 с |\n",
        "| **[Microsoft](https://ppc.land/bing-optimizes-search-speed-with-tensorrt-llm-cutting-model-latency-by-36-percent/)** | Bing Search / Deep Search | TensorRT-LLM | Оптимизация SLM-моделей, уменьшение 95-й процентильной задержки с 4.76 с до 3.03 с и рост пропускной способности на 57% | Снижение затрат и ускорение поисковой выдачи |\n",
        "| **IBM** | Watsonx-платформа и внутренние AI-службы | vLLM | Производственный инференс, активное участие в разработке | Повышенная пропускная способность при снижении VRAM-затрат |\n",
        "| **[LinkedIn](https://blog.vllm.ai/2025/01/10/vllm-2024-wrapped-2025-vision.html)** | AI-функции (рекомендации, сообщения) | vLLM | Генеративные подсказки и контент-ассистенты | Масштабирование на много миллионов пользователей |\n",
        "| **Snowflake, Red Hat, Anyscale и др.** | Облачные и on-prem AI-сервисы | vLLM | Интеграция в хостинг-платформы для LLM-моделей | Упрощённый OpenAI-совместимый API, экономия GPU-часов |\n",
        "| **DeepSeek AI** | DeepSeek-V3 / R1 | TensorRT-LLM | Релиз INT4/BF16 вариантов и официальные примеры | До 8× более быстрый вывод на H100 |\n",
        "| **[NVIDIA Jetson AI Lab](https://www.jetson-ai-lab.com/tensorrt_llm.html)** | Jetson AGX Orin Edge-Inference | TensorRT-LLM | Сборки под ARM-GPU, INT4 Llama-7B | Работа LLM-ов на встраиваемых устройствах |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa250f5-3970-4feb-8f9b-dcf4f4941a7b",
      "metadata": {
        "id": "3fa250f5-3970-4feb-8f9b-dcf4f4941a7b"
      },
      "outputs": [],
      "source": [
        "# уже стартап? Уже можем обслуживать highload? Да, но нужно знать, а как расти и масштабироваться"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a25e76-e485-4fc6-84d3-4f2746988384",
      "metadata": {
        "id": "b5a25e76-e485-4fc6-84d3-4f2746988384"
      },
      "source": [
        "## Конфигурации для масштабирования и параллелизма с TensorRT-LLM и Triton\n",
        "\n",
        "### Стратегии распределения нагрузки\n",
        "\n",
        "#### **1. Tensor Parallelism (TP) — Разделение весов модели**\n",
        "\n",
        "Tensor Parallelism распределяет веса модели между несколькими GPU для моделей, которые не помещаются в память одной карты\n",
        "\n",
        "**Пример конфигурации для LLaMA 70B на 4 GPU:**\n",
        "\n",
        "```bash\n",
        "# Конвертация checkpoint с TP=4\n",
        "python3 convert_checkpoint.py --model_dir ./llama-70b-hf/ \\\n",
        "                             --output_dir ./llama_checkpoint_4gpu_tp4 \\\n",
        "                             --dtype float16 \\\n",
        "                             --tp_size 4\n",
        "\n",
        "# Сборка движка\n",
        "trtllm-build --checkpoint_dir ./llama_checkpoint_4gpu_tp4 \\\n",
        "            --output_dir ./engines/llama-70b/fp16/4-gpu/ \\\n",
        "            --gemm_plugin float16 \\\n",
        "            --gpt_attention_plugin float16 \\\n",
        "            --paged_kv_cache enable\n",
        "```\n",
        "\n",
        "**Переменные окружения для Triton:**\n",
        "```bash\n",
        "export WORLD_SIZE=4\n",
        "export TP_SIZE=4\n",
        "export PP_SIZE=1\n",
        "export GPU_DEVICE_IDS=\"0,1,2,3\"\n",
        "export INSTANCE_COUNT=1\n",
        "```\n",
        "\n",
        "#### **2. Pipeline Parallelism (PP) — Разделение слоев модели**\n",
        "\n",
        "Pipeline Parallelism разделяет слои модели по разным GPU/нодам, создавая конвейер обработки\n",
        "\n",
        "**Пример для LLaMA 70B с TP=4 и PP=2 (8 GPU):**\n",
        "\n",
        "```bash\n",
        "# Конвертация с Pipeline Parallelism\n",
        "python3 convert_checkpoint.py --model_dir ./llama-70b-hf/ \\\n",
        "                             --output_dir ./llama_checkpoint_8gpu_tp4_pp2 \\\n",
        "                             --dtype float16 \\\n",
        "                             --tp_size 4 \\\n",
        "                             --pp_size 2\n",
        "\n",
        "trtllm-build --checkpoint_dir ./llama_checkpoint_8gpu_tp4_pp2 \\\n",
        "            --output_dir ./engines/llama-70b/8-gpu/ \\\n",
        "            --gemm_plugin auto\n",
        "```\n",
        "\n",
        "**Переменные для multi-node deployment:**\n",
        "```bash\n",
        "export WORLD_SIZE=8\n",
        "export TP_SIZE=4\n",
        "export PP_SIZE=2\n",
        "export GPUS_PER_NODE=4\n",
        "export TRITON_BACKEND=\"tensorrtllm\"\n",
        "```\n",
        "\n",
        "#### **3. Data Parallelism — Множественные копии модели**\n",
        "\n",
        "Запуск нескольких независимых копий модели для увеличения пропускной способности\n",
        "\n",
        "**Конфигурация Triton для multiple instances:**\n",
        "```bash\n",
        "export ENGINE_DIR=\"/workspace/trt_engines\"\n",
        "export TOKENIZER_DIR=\"/workspace/local_models/llama-3.2-7B-Instruct\"\n",
        "export INSTANCE_COUNT=4  # 4 копии модели\n",
        "export MAX_BATCH_SIZE=8\n",
        "export GPU_DEVICE_IDS=\"0,1,2,3\"  # Разные GPU для каждой копии\n",
        "```\n",
        "\n",
        "### **Продвинутая оптимизация KV Cache**\n",
        "\n",
        "#### **1. Базовая конфигурация Paged KV Cache**\n",
        "\n",
        "```bash\n",
        "export MAX_TOKENS_IN_PAGED_KV_CACHE=8192\n",
        "export MAX_ATTENTION_WINDOW_SIZE=4096\n",
        "export KV_CACHE_FREE_GPU_MEM_FRACTION=0.7\n",
        "export ENABLE_KV_CACHE_REUSE=True\n",
        "export PAGED_KV_CACHE=enable\n",
        "```\n",
        "\n",
        "#### **2. KV Cache Offloading на CPU память**\n",
        "\n",
        "Для поддержки более длинных контекстов\n",
        "\n",
        "**TensorRT-LLM конфигурация:**\n",
        "```bash\n",
        "# При сборке движка\n",
        "trtllm-build --checkpoint_dir ./checkpoint \\\n",
        "            --output_dir ./engines \\\n",
        "            --paged_kv_cache enable \\\n",
        "            --kv_cache_host_memory_bytes 32000000000  # 32GB в host памяти\n",
        "```\n",
        "\n",
        "**Triton переменные для host offloading:**\n",
        "```bash\n",
        "export ENABLE_KV_CACHE_HOST_OFFLOAD=1\n",
        "export KV_CACHE_HOST_MEM_FRACTION=0.2  # 20% host памяти для KV кэша\n",
        "export KV_CACHE_FREE_GPU_MEM_FRACTION=0.5  # Остальное в GPU\n",
        "```\n",
        "\n",
        "**Для GH200/GB200 с unified memory:**\n",
        "```bash\n",
        "export NIM_ENABLE_KV_CACHE_HOST_OFFLOAD=1\n",
        "export NIM_KV_CACHE_HOST_MEM_FRACTION=0.15\n",
        "```\n",
        "\n",
        "### Комплексные конфигурации для различных сценариев\n",
        "\n",
        "#### **Сценарий 1: Высокая пропускная способность (Multi-Instance)**\n",
        "\n",
        "**config.pbtxt для ensemble модели:**\n",
        "```protobuf\n",
        "name: \"ensemble_model\"\n",
        "platform: \"ensemble\"\n",
        "max_batch_size: 256\n",
        "\n",
        "instance_group [\n",
        "  {\n",
        "    count: 4\n",
        "    kind: KIND_GPU\n",
        "    gpus: [ 0, 1, 2, 3 ]\n",
        "  }\n",
        "]\n",
        "\n",
        "ensemble_scheduling {\n",
        "  step [\n",
        "    {\n",
        "      model_name: \"preprocessing\"\n",
        "      model_version: -1\n",
        "      input_map {\n",
        "        key: \"INPUT_TEXT\"\n",
        "        value: \"input_text\"\n",
        "      }\n",
        "      output_map {\n",
        "        key: \"INPUT_IDS\"\n",
        "        value: \"preprocessed_input\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      model_name: \"tensorrt_llm\"\n",
        "      model_version: -1\n",
        "      input_map {\n",
        "        key: \"input_ids\"\n",
        "        value: \"preprocessed_input\"\n",
        "      }\n",
        "      output_map {\n",
        "        key: \"output_ids\"\n",
        "        value: \"llm_output\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Сценарий 2: Низкая задержка с оптимизацией**\n",
        "\n",
        "```bash\n",
        "# Переменные для минимальной задержки\n",
        "export DECOUPLED_MODE=true  # Стриминг ответа\n",
        "export MAX_QUEUE_DELAY_MICROSECONDS=0\n",
        "export BATCHING_STRATEGY=\"inflight_fused_batching\"\n",
        "export MAX_BATCH_SIZE=4  # Меньший батч для латентности\n",
        "export ENABLE_KV_CACHE_REUSE=True\n",
        "export MAX_NUM_TOKENS=2048\n",
        "```\n",
        "\n",
        "#### **Сценарий 3: Экстремальное масштабирование (Multi-node)**\n",
        "\n",
        "**Для распределенного развертывания:**\n",
        "```bash\n",
        "# Node 1\n",
        "export WORLD_SIZE=16\n",
        "export TP_SIZE=8\n",
        "export PP_SIZE=2\n",
        "export GPUS_PER_NODE=8\n",
        "export NODE_RANK=0\n",
        "export MASTER_ADDR=\"node1_ip\"\n",
        "export MASTER_PORT=\"29500\"\n",
        "\n",
        "# Node 2\n",
        "export WORLD_SIZE=16\n",
        "export TP_SIZE=8  \n",
        "export PP_SIZE=2\n",
        "export GPUS_PER_NODE=8\n",
        "export NODE_RANK=1\n",
        "export MASTER_ADDR=\"node1_ip\"\n",
        "export MASTER_PORT=\"29500\"\n",
        "```\n",
        "\n",
        "**Запуск Triton с MPI:**\n",
        "```bash\n",
        "python3 /app/scripts/launch_triton_server.py \\\n",
        "    --world_size=16 \\\n",
        "    --model_repo=/models\n",
        "```\n",
        "\n",
        "### **Конфигурации для квантования**\n",
        "\n",
        "#### **FP8 Quantization (Hopper архитектура)**\n",
        "\n",
        "```bash\n",
        "# При конвертации checkpoint\n",
        "python3 convert_checkpoint.py --model_dir ./model \\\n",
        "                             --output_dir ./checkpoint \\\n",
        "                             --dtype float16 \\\n",
        "                             --quantization fp8\n",
        "\n",
        "# При сборке движка\n",
        "trtllm-build --checkpoint_dir ./checkpoint \\\n",
        "            --output_dir ./engines \\\n",
        "            --gemm_plugin fp8 \\\n",
        "            --gpt_attention_plugin fp8\n",
        "```\n",
        "\n",
        "#### **INT8 SmoothQuant**\n",
        "\n",
        "```bash\n",
        "# Переменные для INT8\n",
        "export LOGITS_DATATYPE=\"TYPE_FP32\"\n",
        "export ENCODER_INPUT_FEATURES_DATA_TYPE=\"TYPE_FP16\"\n",
        "export QUANTIZATION_MODE=\"int8_smoothquant\"\n",
        "```\n",
        "\n",
        "### **Мониторинг и отладка**\n",
        "\n",
        "```bash\n",
        "# Переменные для подробных логов\n",
        "export TRITON_LOG_VERBOSE=2\n",
        "export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\n",
        "export NCCL_DEBUG=INFO\n",
        "export NCCL_IB_DISABLE=0  # Для InfiniBand\n",
        "\n",
        "# Метрики производительности\n",
        "export TRITON_ENABLE_METRICS=true\n",
        "export TRITON_METRICS_PORT=8002\n",
        "```\n",
        "\n",
        "### **Практические рекомендации**\n",
        "\n",
        "1. **Выбор стратегии параллелизма:**\n",
        "   - **Single-Node Multi-GPU**: Используйте Tensor Parallelism\n",
        "   - **Multi-Node**: Комбинируйте TP + PP (TP = GPUs per node, PP = количество nodes)\n",
        "   - **Высокая нагрузка**: Data Parallelism с множественными экземплярами\n",
        "\n",
        "2. **Оптимизация памяти:**\n",
        "   - Начните с `KV_CACHE_FREE_GPU_MEM_FRACTION=0.9`\n",
        "   - При нехватке памяти включите host offloading\n",
        "   - Используйте квантование для больших моделей\n",
        "\n",
        "3. **Настройка батчинга:**\n",
        "   - `MAX_BATCH_SIZE` = от 1 (латентность) до 256+ (пропускная способность)\n",
        "   - `MAX_NUM_TOKENS` должно соответствовать вашим typical request размерам\n",
        "   - `inflight_fused_batching` для лучшей утилизации"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b4a52c-9749-4bd0-84ba-867a949c47ff",
      "metadata": {
        "id": "94b4a52c-9749-4bd0-84ba-867a949c47ff"
      },
      "source": [
        "____\n",
        "# Часть 3\n",
        "Ракроем следующие темы\n",
        "- 💰 Экономика self-hosting vs API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179ebb17-d021-4a92-a09c-f20999cf299d",
      "metadata": {
        "id": "179ebb17-d021-4a92-a09c-f20999cf299d"
      },
      "outputs": [],
      "source": [
        "# у нас много GPU, у нас небольшой стартап, а это вообще выгодно и эффективно?\n",
        "# не надо ли было платить за LLM API?\n",
        "# для чего вообще начинать разворачивать свою систему?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b58165-dc91-43a4-935b-7da24e6e00fa",
      "metadata": {
        "id": "10b58165-dc91-43a4-935b-7da24e6e00fa"
      },
      "source": [
        "### Вводные для расчетов\n",
        "- **API стоимость:** 200 руб за 1 млн токенов  \n",
        "- **Self-hosting:** фиксированная стоимость 40,000 руб/месяц независимо от объема\n",
        "\n",
        "| Объем (млн токенов) | API стоимость | Self-hosting | Экономия |\n",
        "|---------------------|---------------|--------------|----------|\n",
        "| 1                   | 200 руб       | 40,000 руб   | -39,800 руб |\n",
        "| 50                  | 10,000 руб    | 40,000 руб   | -30,000 руб |\n",
        "| 100                 | 20,000 руб    | 40,000 руб   | -20,000 руб |\n",
        "| 200                 | 40,000 руб    | 40,000 руб   | 0 руб |\n",
        "| 500                 | 100,000 руб   | 40,000 руб   | +60,000 руб |\n",
        "\n",
        "### Альтернативные конфигурации GPU\n",
        "\n",
        "**Сравнение различных GPU-решений:**\n",
        "- **RTX 4090 24GB:** 40,000 руб/мес → точка окупаемости 200 млн токенов\n",
        "- **A100 40GB:** 105,372 руб/мес → точка окупаемости 526.9 млн токенов\n",
        "\n",
        "### 🎯 Когда выбирать self-hosting?\n",
        "\n",
        "**Self-hosting выгоден когда:**\n",
        "- ✅ **Высокий объем**: > 200M токенов/день\n",
        "- ✅ **Предсказуемая нагрузка**: Стабильный трафик\n",
        "- ✅ **Data privacy**: Критична конфиденциальность\n",
        "- ✅ **Кастомизация**: Нужен fine-tuning, специфические optimizations\n",
        "- ✅ **Latency requirements**: < 100ms TTFT\n",
        "\n",
        "**API выгоден когда:**\n",
        "- ✅ **Низкий объем**: < 10M токенов/день\n",
        "- ✅ **Нерегулярная нагрузка**: Spike в использовании\n",
        "- ✅ **Быстрый MVP**: Нужно быстро запустить продукт\n",
        "- ✅ **Ограниченные ресурсы**: Нет команды для поддержки инфраструктуры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0b94cf-202a-491c-96f0-ffca91d04754",
      "metadata": {
        "id": "fc0b94cf-202a-491c-96f0-ffca91d04754",
        "outputId": "26fe04ac-6dbc-40a9-96ec-05803b868af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "well done\n"
          ]
        }
      ],
      "source": [
        "print('well done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd1e9e5-0094-4df8-a1a2-cac9a9e77fce",
      "metadata": {
        "id": "1bd1e9e5-0094-4df8-a1a2-cac9a9e77fce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_jupyter",
      "language": "python",
      "name": "venv_jupyter"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}